%!TEX root = report.tex

\chapter{Experiments}
\label{chp:experiments}
In this chapter we detail the experiments performed on the model described in
\fullref{chp:theorystuff} and discuss the respective results.

\section{First experiment}
\label{subsec:first}

% documentazione wrapfig: https://ctan.mirror.garr.it/mirrors/ctan/macros/latex/contrib/wrapfig/wrapfig-doc.pdf
% lo segno perché il primo parametro opzionale può tornare utile
\begin{wrapfigure}[15]{r}{0.5\textwidth}
    \centering
    \vspace{-20pt}
    \includegraphics[width=0.5\textwidth]{400_loss}
    \caption{Initial experiment loss (400 epochs on Wiki)}
    \label{fig:400_loss}
\end{wrapfigure}

An initial experiment was performed by training the C3AE model for
400 epochs on the Wiki dataset as a means to test the \texttt{tensorflow} environment
and the functioning of the implemented C3AE model within it.

The evolution of training and validation loss is shown in \autoref{fig:400_loss}.
The experiment ran to completition with no runtime errors, and
it can be observed that the model was able to decrease its loss,
and that such loss reached an asymptote around epoch 50.

For this reason, we concluded that a training time of 400 epochs
is too much for this model and decided to set the epoch limit to 100 for
all following experiments, assuming that they would have a similar
evolution to this one and therefore all significant improvement
would happen much before the 100\textsuperscript{th} epoch,
in order to reduce the experiments' computation time.

\section{Performance evaluation over multiple datasets}

All the following experiments have been performed on datasets processed with the 
augmentation techniques described in \fullref{subsec:augmentation} unless stated otherwise.

\newpage

\subsection{Wiki}
\label{subsec:wiki}

\begin{wrapfigure}[15]{r}{0.5\textwidth}
  \centering
  \vspace{-20pt}
  \includegraphics[width=0.5\textwidth]{full_model_mae}
  \caption{MAE on Wiki dataset (100 epochs)}
  \label{fig:wiki_loss}
\end{wrapfigure}

The first and main dataset used in our experiments has been Wiki.
As shown in the loss graph in \autoref{fig:wiki_loss} the learning curve is
correct and the best MAE value obtained in this experiment is 6.79 years. 
When compared to the result in the original paper, which claims to have reached 
a MAE of 6.44 on the same dataset, we could say that we have pretty much achieved
a state-of-the-art performance.

However, this result only takes in consideration Wiki images for both training
and validation. So we tested the output of this experiment with the FGNET test set,
and found a much higher MAE of 18.1 years.
The main reason for this is that the Wiki dataset is mostly composed by images
of adults and elders and features almost no children, while FGNET has a much lower
age on average and comprises even newborns with a declared age of 0 years. 
The result is that the model in this experiment always overshoots the age of the 
subjects and thus we obtain a high error.

\subsection{UTK}
\label{subsec:utk}

\begin{wrapfigure}[12]{r}{0.5\textwidth}
  \centering
  \vspace{-20pt}
  \includegraphics[width=0.5\textwidth]{utk_mae}
  \caption{MAE on UTK dataset (100 epochs)}
  \label{fig:utk_loss}
\end{wrapfigure}

In this experiment the starting conditions and parameters are the same as the 
previous one, but the dataset this time is UTK. The number of images is one third
of Wiki, but it covers better the whole range of ages from 0 to over 100.
As a matter of fact the validation MAE (computed on the UTK set itself) is slightly
higher than before at 8.67, but the test MAE on FGNET is almost halved, at 9.79 years.

The conclusion is that UTK is a better dataset for our experiments, but we can still 
use Wiki, as seen on the next experiment.

\subsection{Wiki + UTK}
\label{subsec:wikiutk}

\begin{wrapfigure}[11]{r}{0.5\textwidth}
  \centering
  \vspace{-20pt}
  \includegraphics[width=0.5\textwidth]{wiki+utk_200_mae}
  \caption{MAE on Wiki+UTK (100 epochs)}
  \label{fig:wiki+utk_loss}
\end{wrapfigure}

This third experiment combines the previous two. We started by pre-training a model from 
scratch with the Wiki dataset, and then we took the output of this process and further 
trained it for another 100 epochs on the UTK dataset. In this way we hoped to combine the
scale of the first dataset with the completeness of the second to obtain a model that
outperforms the previous two. And indeed, with a final validation MAE of 8.23 and a test
MAE of 8.64 years on FGNET this has proven to be our best result yet.

\section{Ablation Study}
\label{sec:ablation_study}
A separate set of experiments was performed to study the impact on performance
of the following components of the model and the training process:
the context module and the cascade module of the C3AE model,
and the training data augmentation.

We trained the following variants of the full C3AE model:

\begin{itemize}
  \item \textit{Full model}: the standard model with no changes,
    to serve as a benchmark against the other variants.
  \item \textit{No augmentation}: the data augmentation transformations
    on the training data are disabled.
  \item \textit{No context}: the context module of C3AE is excluded.
    Therefore, only one crop is given as input to the model.
  \item \textit{No cascade}: the cascade module of C3AE is excluded.
    Consequently, this variant does not compute any KL Divergence and 
    outputs only the final age estimation.
  \item \textit{No context and no cascade}: both modules are disabled.
\end{itemize}

Each variant was trained for 100 epochs on the Wiki dataset, so every ablation experiment
results must be compared to the baseline results of the one described in \fullref{subsec:wiki}.

\subsection{No augmentation}
\label{subsec:no_augmentation}

In this first ablation study we removed the whole augmentation process described 
in \fullref{subsec:augmentation}. The validation MAE increases by 23\% compared to
the the Wiki full model experiments, and the test MAE by not less that 30\%. 
We conclude that the augmentation phase implemented by us has been highly beneficial 
to the full model performance.

\subsection{No context}
\label{subsec:no_context}

After the removal of the context module, the model retained its performance
if we consider only the validation results (MAE = 6.88). However when examining
the test results on FGNET they are rather worse then before, with the test MAE going 
from 18.11 to 20.73. This performance difference proves us that considering image cuts
with three different levels of details instead of only one and combining their results 
is useful to the full model performance.

\subsection{No cascade}
\label{subsec:no_cascade}

When we take out the cascade module we the only metric that contributes to the total loss is
the MAE, since we lose the output with the age distribution and also the KLD loss.
The direct exstimation of an age from the image in input is quite a harder task, judging
by the results of this experiment.
The validation MAE is by far the highest found so far, at 12.6 years, and the test MAE on FGNET 
isn't great either, reaching 18.67. Once again, we can interpret this performance drop as a 
proof that also the cascade module is essential to obtain a good result in the full model.

\subsection{Full ablation}
\label{subsec:full_ablation}

The absence of both the previous modules results unsurprisingly in a performance comparable 
to the worst of the two. The validation and test MAE values are in fact aligned to the MAEs 
returned by the "no cascade" model.

\begin{figure}
  \centering
  \subfloat[MAE without augmentation]{
    \includegraphics[width=65mm]{no_augm_mae}
    \label{fig:no_augm}
  }
  \subfloat[MAE without context]{
    \includegraphics[width=65mm]{no_context_mae}
    \label{fig:no_context}
  }
  \hspace{0mm}
  \subfloat[MAE without cascade]{
    \includegraphics[width=65mm]{no_cascade_mae}
    \label{fig:no_cascade}
  }
  \subfloat[MAE without either]{
    \includegraphics[width=65mm]{full_ablation_mae}
    \label{fig:full_ablation}
  }
  \caption{Ablation Study MAE}
\end{figure}